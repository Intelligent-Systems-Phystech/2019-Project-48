{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "from scipy.misc import imresize\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_tensor_type('torch.FloatTensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mnist_data import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot_labels(a):\n",
    "    b = np.zeros((a.size, a.max()+1))\n",
    "    b[np.arange(a.size),a] = 1\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myDataSet(Dataset):\n",
    "    def __init__(self, objects, labels, transform=None):\n",
    "        assert len(objects) == len(labels)\n",
    "        self.X = objects\n",
    "        self.y = labels\n",
    "        self.len = len(objects)\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.X[idx], self.y[idx]\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Downsample(object):\n",
    "    def __init__(self, p_down):\n",
    "        self.p_down = p_down\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        image, label = sample\n",
    "        restored_image = image.reshape(28,28)\n",
    "        image = imresize(restored_image, self.p_down, mode='F').ravel()\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample\n",
    "\n",
    "        return torch.from_numpy(image).double(), torch.from_numpy(label).double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_val, y_val, X_test, y_test = load_dataset()\n",
    "X_train, X_test, X_val = [a.reshape((a.shape[0], -1)) for a in [X_train, X_test, X_val]]\n",
    "y_train, y_test, y_val = [onehot_labels(a) for a in [y_train, y_test, y_val]]\n",
    "\n",
    "shuffled_train_indices = np.arange(X_train.shape[0])\n",
    "np.random.shuffle(shuffled_train_indices)\n",
    "X_train, y_train = X_train[shuffled_train_indices], y_train[shuffled_train_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = myDataSet(X_test, y_test, transform=transforms.Compose([ToTensor()]))\n",
    "data_val = myDataSet(X_val, y_val, transform=transforms.Compose([ToTensor()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_val[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions_on_dataset(model, dataset, batch_size=None, compute_accuracy=False, num_output=None):\n",
    "    def _target_predictions(output, num_output):\n",
    "        if num_output is None:\n",
    "            return output\n",
    "        else:\n",
    "            return output[num_output]\n",
    "        \n",
    "    if batch_size is None:\n",
    "        batch_size = len(dataset)\n",
    "    data_loader = DataLoader(dataset, \n",
    "                             batch_size=batch_size,\n",
    "                             shuffle=False, \n",
    "                             num_workers=2)\n",
    "    raw_predictions = np.zeros((len(dataset), dataset[0][1].shape[0]))\n",
    "    all_predictions = np.zeros(len(dataset))\n",
    "    all_labels = np.zeros_like(all_predictions)\n",
    "    \n",
    "#     print(batch_size)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        cur_start = 0\n",
    "        for batch, labels in data_loader:\n",
    "            _raw_prediction = _target_predictions(model(batch.cuda()), num_output)\n",
    "            raw_predictions[cur_start:cur_start+batch_size] = np.array(_raw_prediction)\n",
    "#             print(np.array(_raw_prediction).shape)\n",
    "            predictions = np.array(_raw_prediction)\n",
    "#             print(predictions.shape)\n",
    "            predictions = np.argmax(predictions, axis=1)\n",
    "            labels = np.argmax(np.array(labels), axis=1)\n",
    "            all_predictions[cur_start:cur_start+batch_size] = predictions\n",
    "            all_labels[cur_start:cur_start+batch_size] = labels\n",
    "            cur_start += batch_size\n",
    "    if compute_accuracy:\n",
    "        val_acc = np.mean(all_labels==all_predictions, dtype='float')\n",
    "        return raw_predictions, all_labels, val_acc\n",
    "    else:\n",
    "        return raw_predictions, all_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(_input, target, size_average=True):\n",
    "    \"\"\" Cross entropy that accepts soft targets\n",
    "    Args:\n",
    "         pred: predictions for neural network\n",
    "         targets: targets, can be soft\n",
    "         size_average: if false, sum is returned instead of mean\n",
    "\n",
    "    Examples::\n",
    "\n",
    "        _input = torch.FloatTensor([[1.1, 2.8, 1.3], [1.1, 2.1, 4.8]])\n",
    "        _input = torch.autograd.Variable(out, requires_grad=True)\n",
    "\n",
    "        target = torch.FloatTensor([[0.05, 0.9, 0.05], [0.05, 0.05, 0.9]])\n",
    "        target = torch.autograd.Variable(y1)\n",
    "        loss = cross_entropy(_input, target)\n",
    "        loss.backward()\n",
    "    \"\"\"\n",
    "    logsoftmax = torch.nn.LogSoftmax(-1)\n",
    "    if size_average:\n",
    "        return torch.mean(torch.sum(-target * logsoftmax(_input), dim=1))\n",
    "    else:\n",
    "        return torch.sum(torch.sum(-target * logsoftmax(_input), dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLP(d, m, q):\n",
    "    model = torch.nn.Sequential()\n",
    "    model.add_module('d1', torch.nn.Linear(d, m))\n",
    "    model.add_module('a1', torch.nn.ReLU())\n",
    "    model.add_module('d2', torch.nn.Linear(m, m))\n",
    "    model.add_module('a2', torch.nn.ReLU())\n",
    "    model.add_module('d3', torch.nn.Linear(m, q))\n",
    "    model.add_module('a3', torch.nn.Softmax(-1))\n",
    "    \n",
    "    opt = torch.optim.RMSprop(model.parameters(), lr=1e-3)\n",
    "    \n",
    "    loss = cross_entropy\n",
    "    \n",
    "    return (model, opt, loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoOutputsNN(torch.nn.Module):\n",
    "    def __init__(self, d, m, q):\n",
    "        super(TwoOutputsNN, self).__init__()\n",
    "        self._lin1 = torch.nn.Linear(d, m)\n",
    "        self._act1 = torch.nn.ReLU()\n",
    "        self._lin2 = torch.nn.Linear(m, m)\n",
    "        self._act2 = torch.nn.ReLU()\n",
    "        self._lin3 = torch.nn.Linear(m, q)\n",
    "        self._out_softmax = torch.nn.Softmax(-1)\n",
    "        \n",
    "        self._queue = [\n",
    "            self._lin1,\n",
    "            self._act1,\n",
    "            self._lin2,\n",
    "            self._act2,\n",
    "            self._lin3\n",
    "        ]\n",
    "        \n",
    "    def forward(self, x):\n",
    "        result = x\n",
    "        for layer in self._queue:\n",
    "            result = layer(result)\n",
    "        \n",
    "        out1 = self._out_softmax(result)\n",
    "        out2 = result\n",
    "        return out1, out2\n",
    "    \n",
    "def get_teacher(d, m, q):\n",
    "    model = TwoOutputsNN(d, m, q)\n",
    "    opt = torch.optim.RMSprop(model.parameters(), lr=1e-3)\n",
    "    \n",
    "    loss = cross_entropy\n",
    "    \n",
    "    return (model, opt, loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_loss(base_loss,l):\n",
    "    def loss_function(y_true, y_pred):\n",
    "        return l*base_loss(y_true,y_pred)\n",
    "    return loss_function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Dist(d, m, q, L):\n",
    "    def _hard_loss(_input, target, L):\n",
    "        return (1.-L)*cross_entropy(_input, target)\n",
    "\n",
    "    def _soft_loss(_input, target, L):\n",
    "        return L*cross_entropy(_input, target)\n",
    "\n",
    "    \n",
    "    model = torch.nn.Sequential()\n",
    "    model.add_module('d1', torch.nn.Linear(d, m))\n",
    "    model.add_module('a1', torch.nn.ReLU())\n",
    "    model.add_module('d2', torch.nn.Linear(m, m))\n",
    "    model.add_module('a2', torch.nn.ReLU())\n",
    "    model.add_module('d3', torch.nn.Linear(m, q))\n",
    "    model.add_module('a3', torch.nn.Softmax(-1))\n",
    "    \n",
    "    hard_loss = _hard_loss\n",
    "    soft_loss = _soft_loss\n",
    "\n",
    "    \n",
    "    opt = torch.optim.RMSprop(model.parameters(), lr=1e-3)\n",
    "#     opt = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "\n",
    "    \n",
    "    \n",
    "    return (model, opt, hard_loss, soft_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = X_train[0].reshape(-1).shape[0]\n",
    "m = 20\n",
    "q = y_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, opt, loss = get_teacher(d, m, q)\n",
    "model = model.double()\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = N\n",
    "data_train = myDataSet(X_train[:N], y_train[:N], transform=transforms.Compose([ToTensor()]))\n",
    "\n",
    "dataloader_train = DataLoader(data_train, \n",
    "                              batch_size=4,\n",
    "                              shuffle=True, \n",
    "                              num_workers=1)\n",
    "\n",
    "_t = tqdm.tnrange(200, desc='epoch')\n",
    "for epoch in _t:\n",
    "    sum_loss = np.zeros(len(dataloader_train), dtype='float64')\n",
    "    for idx, (batch, label) in enumerate(tqdm.tqdm_notebook(dataloader_train, leave=False)):\n",
    "        batch = batch.cuda()\n",
    "        label = label.cuda()\n",
    "        # Step 1. Remember that PyTorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        predictions = model(batch)[0]\n",
    "        loss_value = loss(predictions, label)\n",
    "        loss_value.backward()\n",
    "        opt.step()\n",
    "        sum_loss[idx] = loss_value\n",
    "    if not epoch % 10:\n",
    "        val_acc = get_predictions_on_dataset(model, data_val, compute_accuracy=True, num_output=0)[-1]\n",
    "    _t.set_postfix(sum_loss=sum_loss.mean(), val_acc=val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "a = get_predictions_on_dataset(model, data_test, compute_accuracy=True, num_output=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test_acc = a[-1]\n",
    "a[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = get_predictions_on_dataset(model, data_train, compute_accuracy=True, num_output=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_probs_hist(probs):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    for idx, prob in enumerate(probs):\n",
    "        plt.bar(np.arange(len(prob)), prob, alpha=1./len(probs))\n",
    "    plt.ylim((0., 1.))\n",
    "    plt.xticks(np.arange(len(prob)))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# for T in [1,2,5,10,20,50]:\n",
    "#     _ls = ['dashed', 'dotted']\n",
    "#     _fc = [(0, 0, 1, 0.5), (1, 0, 0, 0.3)]\n",
    "#     f, axarr = plt.subplots(3, 3, figsize=(16,10))\n",
    "#     axarr[0, 1].set_title('T = {}'.format(T))\n",
    "#     for _i in range(9):\n",
    "#         labels_soften = F.softmax(torch.Tensor(b[0][_i]/T), -1)\n",
    "#         probs = [labels_soften, data_train[_i][1]]\n",
    "#         ix, iy = int(int(_i)/3), (_i % 3)\n",
    "\n",
    "#         for idx, prob in enumerate(probs):\n",
    "#             axarr[ix, iy].bar(np.arange(len(prob)),prob, edgecolor='black', lw=1., ls=_ls[idx], fc=_fc[idx])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.softmax(torch.Tensor(b[:3][0]/10), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del num_try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "transformed_data_train = myDataSet(X_train[:N], y_train[:N], transform=transforms.Compose([\n",
    "                                                                                            Downsample(0.25),\n",
    "                                                                                            ToTensor()\n",
    "                                                                                          ]))\n",
    "transformed_dataloader_train = DataLoader(transformed_data_train, \n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False, \n",
    "                                          num_workers=1)\n",
    "\n",
    "transformed_data_val = myDataSet(X_val, y_val, transform=transforms.Compose([\n",
    "                                                                             Downsample(0.25),\n",
    "                                                                             ToTensor()\n",
    "                                                                            ]))\n",
    "\n",
    "transformed_data_test = myDataSet(X_test, y_test, transform=transforms.Compose([\n",
    "                                                                                Downsample(0.25),\n",
    "                                                                                ToTensor()\n",
    "                                                                                ]))\n",
    "\n",
    "d = transformed_data_train[0][0].shape[0]\n",
    "\n",
    "model_predictions = get_predictions_on_dataset(model, data_train, 50, num_output=1)[0]\n",
    "model_predictions = torch.Tensor(model_predictions).double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_save = {\n",
    "    'transformed_data_train': transformed_data_train,\n",
    "    'transformed_dataloader_train': transformed_dataloader_train,\n",
    "    'transformed_data_val': transformed_data_val,\n",
    "    'transformed_data_test': transformed_data_test,\n",
    "    'model_predictions': model_predictions,\n",
    "    'model': model,\n",
    "    'data_test': data_test,\n",
    "    'data_val': data_val,\n",
    "    'model_test_acc': model_test_acc\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(to_save, 'saved_objects_mnist_{}_2.pcl'.format(N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for num_try in tqdm.tnrange(0, 10):\n",
    "    iofile = open('torch_version_logs/new_log_mnist_{}.txt'.format(num_try), 'w')\n",
    "\n",
    "    # \n",
    "    # \n",
    "    for T in tqdm.tqdm_notebook([1,2,5,10,20,50], desc='t loop', leave=False):\n",
    "        for L in tqdm.tqdm_notebook([0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0], desc='l loop', leave=False):\n",
    "            labels_soften = F.softmax(model_predictions/T, -1)\n",
    "    #         print(labels_soften)\n",
    "\n",
    "\n",
    "            student, student_opt, hard_loss, soft_loss = Dist(d, m, q, L)\n",
    "            student.double()\n",
    "            student.cuda()\n",
    "\n",
    "            _t = tqdm.tnrange(151, leave=False)\n",
    "            for epoch in _t:\n",
    "                cur_start = 0\n",
    "                loss_hist = []\n",
    "                for batch, label in tqdm.tqdm_notebook(transformed_dataloader_train, leave=False):\n",
    "            #         print(cur_start, batch_size, labels_soften[cur_start:cur_start+batch_size].shape)\n",
    "                    batch = batch.cuda()\n",
    "                    label = label.cuda()\n",
    "                    # Step 1. Remember that PyTorch accumulates gradients.\n",
    "                    # We need to clear them out before each instance\n",
    "                    student.zero_grad()\n",
    "\n",
    "                    predictions = student(batch)\n",
    "                    hard_loss = cross_entropy(predictions, label)\n",
    "                    soft_loss = cross_entropy(predictions, labels_soften[cur_start:cur_start+batch_size].cuda())\n",
    "\n",
    "                    total_loss = (1.-L)*hard_loss + L*soft_loss\n",
    "                    total_loss.backward()\n",
    "            #                 soft_loss.backward()\n",
    "                    loss_hist.append(np.mean(np.array(total_loss.detach())))\n",
    "\n",
    "\n",
    "                    student_opt.step()\n",
    "                    cur_start += batch_size\n",
    "\n",
    "            #     b = student.state_dict()\n",
    "            #     print(all(np.array(b['_lin1.weight']).ravel() == np.array(a['_lin1.weight']).ravel()))\n",
    "    #             if epoch % 25 == 0:\n",
    "    #                 val_acc = get_predictions_on_dataset(student, transformed_data_val, compute_accuracy=True)[-1]\n",
    "                _t.set_postfix(val_acc=val_acc, mean_loss = np.mean(loss_hist[-50:]))\n",
    "\n",
    "            #         for param in student.parameters():\n",
    "            #             print(param.grad.data.sum())\n",
    "            #         import pdb; pdb.set_trace()\n",
    "            acc_student = get_predictions_on_dataset(student, transformed_data_test, compute_accuracy=True)[-1]\n",
    "            iofile.write(str([N, T, L, acc_student])+'\\n')\n",
    "\n",
    "    iofile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix = '200'\n",
    "N_grid = [str(500)]*12 + [str(300)]*12\n",
    "num_try_grid = [str(i) for i in range(12)]*2\n",
    "parameters = [' '.join((str(i%2+1), N_grid[i], num_try_grid[i], suffix)) for i in range(24)]\n",
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict = dict()\n",
    "for T, L in [(1., 0.), (5., 0.5)]:\n",
    "    result_dict[(T, L)] = []\n",
    "    for _i in tqdm.tnrange(10):\n",
    "        student, student_opt, hard_loss, soft_loss = Dist(d, m, q, L)\n",
    "        student.double()\n",
    "        student.cuda()\n",
    "        labels_soften = F.softmax(model_predictions/T, -1)\n",
    "\n",
    "        _t = tqdm.tnrange(151, leave=False)\n",
    "        for epoch in _t:\n",
    "            cur_start = 0\n",
    "            loss_hist = []\n",
    "            for batch, label in tqdm.tqdm_notebook(transformed_dataloader_train, leave=False):\n",
    "        #         print(cur_start, batch_size, labels_soften[cur_start:cur_start+batch_size].shape)\n",
    "                batch = batch.cuda()\n",
    "                label = label.cuda()\n",
    "                # Step 1. Remember that PyTorch accumulates gradients.\n",
    "                # We need to clear them out before each instance\n",
    "                student.zero_grad()\n",
    "\n",
    "                predictions = student(batch)\n",
    "                hard_loss = cross_entropy(predictions, label)\n",
    "                soft_loss = cross_entropy(predictions, labels_soften[cur_start:cur_start+batch_size].cuda())\n",
    "\n",
    "                total_loss = (1.-L)*hard_loss + L*soft_loss\n",
    "                total_loss.backward()\n",
    "        #                 soft_loss.backward()\n",
    "                loss_hist.append(np.mean(np.array(total_loss.detach())))\n",
    "\n",
    "\n",
    "                student_opt.step()\n",
    "                cur_start += batch_size\n",
    "\n",
    "        #     b = student.state_dict()\n",
    "        #     print(all(np.array(b['_lin1.weight']).ravel() == np.array(a['_lin1.weight']).ravel()))\n",
    "            if epoch % 25 == 0:\n",
    "                val_acc = get_predictions_on_dataset(student, transformed_data_val, compute_accuracy=True)[-1]\n",
    "            _t.set_postfix(val_acc=val_acc, mean_loss = np.mean(loss_hist[-50:]))\n",
    "\n",
    "        #         for param in student.parameters():\n",
    "        #             print(param.grad.data.sum())\n",
    "        #         import pdb; pdb.set_trace()\n",
    "        acc_student = get_predictions_on_dataset(student, transformed_data_test, compute_accuracy=True)[-1]\n",
    "        result_dict[(T, L)].append(acc_student)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(map(np.mean, result_dict.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, val in result_dict.items():\n",
    "    print(key, np.mean(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, val in result_dict.items():\n",
    "    print(key, np.mean(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_predictions_on_dataset(model, data_test, compute_accuracy=True, num_output=0)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student, student_opt, hard_loss, soft_loss = Dist(d, m, q, L)\n",
    "student.double()\n",
    "student.cuda()\n",
    "\n",
    "\n",
    "_t = tqdm.tnrange(151, leave=False)\n",
    "for epoch in _t:\n",
    "    cur_start = 0\n",
    "    loss_hist = []\n",
    "    for batch, label in tqdm.tqdm_notebook(transformed_dataloader_train, leave=False):\n",
    "#         print(cur_start, batch_size, labels_soften[cur_start:cur_start+batch_size].shape)\n",
    "        batch = batch.cuda()\n",
    "        label = label.cuda()\n",
    "        # Step 1. Remember that PyTorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        student.zero_grad()\n",
    "\n",
    "        predictions = student(batch)\n",
    "        hard_loss = cross_entropy(predictions, label)\n",
    "#         soft_loss = cross_entropy(predictions, labels_soften[cur_start:cur_start+batch_size].cuda())\n",
    "\n",
    "#         total_loss = (1.-L)*hard_loss + L*soft_loss\n",
    "#         total_loss.backward()\n",
    "        hard_loss.backward()\n",
    "#         loss_hist.append(np.mean(np.array(total_loss.detach())))\n",
    "\n",
    "\n",
    "        student_opt.step()\n",
    "        cur_start += batch_size\n",
    "\n",
    "#     b = student.state_dict()\n",
    "#     print(all(np.array(b['_lin1.weight']).ravel() == np.array(a['_lin1.weight']).ravel()))\n",
    "    if epoch % 25 == 0:\n",
    "        val_acc = get_predictions_on_dataset(student, transformed_data_val, compute_accuracy=True)[-1]\n",
    "    _t.set_postfix(val_acc=val_acc, mean_loss = np.mean(loss_hist[-50:]))\n",
    "\n",
    "#         for param in student.parameters():\n",
    "#             print(param.grad.data.sum())\n",
    "#         import pdb; pdb.set_trace()\n",
    "acc_student = get_predictions_on_dataset(student, transformed_data_test, compute_accuracy=True)[-1]\n",
    "result_dict[(T, L)].append(acc_student)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = np.zeros((len(logs), 4), dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = '[\\d+, \\d+\\.?\\d+, \\d+. \\d ]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_logs(log_str):\n",
    "    log_str = log_str.replace('[', '').replace(']', '').replace('\\n', '').replace(' ', '')\n",
    "    return np.array([float(x) for x in log_str.split(',')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = _parse_logs(raw_logs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = np.array(list(map(_parse_logs, raw_logs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_coords = dict()\n",
    "for T in list(set(logs[:, 1])):\n",
    "    line_coords[T] = logs[logs[:, 1] == T][:, 2:]\n",
    "\n",
    "plt.figure(figsize=(16,10))\n",
    "for T, line_coords in line_coords.items():\n",
    "    plt.plot(line_coords[:, 0], line_coords[:, 1], label=T)\n",
    "\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.xlabel('imitation parameter')\n",
    "plt.ylabel('test accuracy')\n",
    "plt.title('MNIST 500 images')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_soften = F.softmax(model_predictions/T, -1)\n",
    "print(labels_soften)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy(predictions, labels_soften[cur_start:cur_start+batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_soften[cur_start:cur_start+batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_soften.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_data_test = myDataSet(X_test, y_test, transform=transforms.Compose([\n",
    "                                                                             Downsample(0.25),\n",
    "                                                                             ToTensor()\n",
    "                                                                            ]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_predictions_on_dataset(student, transformed_data_test, compute_accuracy=True)[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    num_try += 1\n",
    "except NameError:\n",
    "    num_try = 0\n",
    "    \n",
    "iofile = open('torch_version_logs/log_mnist_{}.txt'.format(num_try), 'w')\n",
    "\n",
    "\n",
    "for T in tqdm.tqdm_notebook([1,2,5,10,20,50], desc='t loop'):\n",
    "    for L in tqdm.tqdm_notebook([0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0], desc='l loop', leave=False):\n",
    "        student, hard_opt, soft_opt, hard_loss, soft_loss = Dist(d, m, q, T, L)\n",
    "        student.double()\n",
    "        student.cuda()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        _t = tqdm.tnrange(5, leave=False)\n",
    "        for epoch in _t:\n",
    "            cur_start = 0\n",
    "            for batch, label in tqdm.tqdm_notebook(transformed_dataloader_train, leave=False):\n",
    "                batch = batch.cuda()\n",
    "                label = label.cuda()\n",
    "                # Step 1. Remember that PyTorch accumulates gradients.\n",
    "                # We need to clear them out before each instance\n",
    "                student.zero_grad()\n",
    "\n",
    "                predictions = student(batch)\n",
    "                hard_loss_value = hard_loss(predictions, label, T, L)\n",
    "                soft_loss_value = soft_loss(predictions, labels_soften[cur_start:cur_start+batch_size].cuda(), L)\n",
    "\n",
    "                hard_loss_value.backward(retain_graph=True)\n",
    "                soft_loss_value.backward()\n",
    "\n",
    "                hard_opt.step()\n",
    "                soft_opt.step()\n",
    "                cur_start += batch_size\n",
    "\n",
    "            val_acc = get_predictions_on_dataset(student, transformed_data_val, compute_accuracy=True)[-1]\n",
    "            _t.set_postfix(val_acc=val_acc)\n",
    "\n",
    "                \n",
    "        acc_student = get_predictions_on_dataset(student, transformed_data_test, compute_accuracy=True)[-1]\n",
    "        iofile.write(str([N, T, L, acc_student])+'\\n')\n",
    "        \n",
    "iofile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tail log_mnist_0.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for rep in range(10):\n",
    "    # random training split\n",
    "    i     = np.random.permutation(ax_tr.shape[0])[0:N]\n",
    "    x_tr  = ax_tr[i]\n",
    "    y_tr  = ay_tr[i]\n",
    "    xs_tr = downsample(x_tr,p_downsample)\n",
    "    x_tr  = x_tr/255.0\n",
    "    xs_tr = xs_tr/255.0\n",
    "\n",
    "    # big mlp\n",
    "    print(x_tr.shape, y_tr.shape)\n",
    "    mlp_big = MLP(x_tr.shape[1],M,y_tr.shape[1])\n",
    "    mlp_big.fit(x_tr, y_tr, nb_epoch=50, verbose=0)\n",
    "    err_big = np.mean(mlp_big.predict_classes(x_te,verbose=0)==np.argmax(y_te,1))\n",
    "\n",
    "    # student mlp\n",
    "    for t in tqdm.tqdm_notebook([1,2,5,10,20,50], desc='t loop'):\n",
    "        for L in tqdm.tqdm_notebook([0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0], desc='l loop', leave=False):\n",
    "            soften = theano.function([mlp_big.layers[0].input], mlp_big.layers[2].output)\n",
    "            ys_tr  = softmax(soften(x_tr),t)\n",
    "            print(ys_tr.shape, y_tr.shape)\n",
    "\n",
    "            mlp_student = get_distillation(xs_tr.shape[1],M,ys_tr.shape[1],t,L)\n",
    "            mlp_student.fit(xs_tr, {'hard':y_tr, 'soft':ys_tr}, nb_epoch=50, verbose=0)\n",
    "            err_student = np.mean(np.argmax(mlp_student.predict({'x':xs_te})['hard'],1)==np.argmax(y_te,1))\n",
    "\n",
    "            line = [N, p_downsample, round(err_big,3), t, L, round(err_student,3)]\n",
    "            outfile.write(str(line)+'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 with tf",
   "language": "python",
   "name": "tf_py3_ipykernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
