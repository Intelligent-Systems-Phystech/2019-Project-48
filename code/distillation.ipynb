{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "from scipy.misc import imresize\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "torch.set_default_tensor_type('torch.FloatTensor')\n",
    "\n",
    "#from mnist_data import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n",
    "num_try = 2\n",
    "n_epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'N' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-e8ad04a13e51>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-e8ad04a13e51>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m     \u001b[0msaved_filename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'saved_objects_mnist_{}.pcl'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m     \u001b[0mfrom_save\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaved_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'N' is not defined"
     ]
    }
   ],
   "source": [
    "def onehot_labels(a):\n",
    "    b = np.zeros((a.size, a.max()+1))\n",
    "    b[np.arange(a.size),a] = 1\n",
    "    return b\n",
    "\n",
    "\n",
    "class myDataSet(Dataset):\n",
    "    def __init__(self, objects, labels, transform=None):\n",
    "        assert len(objects) == len(labels)\n",
    "        self.X = objects\n",
    "        self.y = labels\n",
    "        self.len = len(objects)\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.X[idx], self.y[idx]\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "\n",
    "class Downsample(object):\n",
    "    def __init__(self, p_down):\n",
    "        self.p_down = p_down\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        image, label = sample\n",
    "        restored_image = image.reshape(28,28)\n",
    "        image = imresize(restored_image, self.p_down, mode='F').ravel()\n",
    "        return image, label\n",
    "\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample\n",
    "\n",
    "        return torch.from_numpy(image).double(), torch.from_numpy(label).double()\n",
    "\n",
    "\n",
    "def get_predictions_on_dataset(model, dataset, batch_size=None, compute_accuracy=False, num_output=None):\n",
    "    def _target_predictions(output, num_output):\n",
    "        if num_output is None:\n",
    "            return output\n",
    "        else:\n",
    "            return output[num_output]\n",
    "        \n",
    "    if batch_size is None:\n",
    "        batch_size = len(dataset)\n",
    "    data_loader = DataLoader(dataset, \n",
    "                             batch_size=batch_size,\n",
    "                             shuffle=False, \n",
    "                             num_workers=2)\n",
    "    raw_predictions = np.zeros((len(dataset), dataset[0][1].shape[0]))\n",
    "    all_predictions = np.zeros(len(dataset))\n",
    "    all_labels = np.zeros_like(all_predictions)\n",
    "    \n",
    "#     print(batch_size)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        cur_start = 0\n",
    "        for batch, labels in data_loader:\n",
    "            _raw_prediction = _target_predictions(model(batch.cuda()), num_output)\n",
    "            raw_predictions[cur_start:cur_start+batch_size] = np.array(_raw_prediction)\n",
    "#             print(np.array(_raw_prediction).shape)\n",
    "            predictions = np.array(_raw_prediction)\n",
    "#             print(predictions.shape)\n",
    "            predictions = np.argmax(predictions, axis=1)\n",
    "            labels = np.argmax(np.array(labels), axis=1)\n",
    "            all_predictions[cur_start:cur_start+batch_size] = predictions\n",
    "            all_labels[cur_start:cur_start+batch_size] = labels\n",
    "            cur_start += batch_size\n",
    "    if compute_accuracy:\n",
    "        val_acc = np.mean(all_labels==all_predictions, dtype='float')\n",
    "        return raw_predictions, all_labels, val_acc\n",
    "    else:\n",
    "        return raw_predictions, all_labels\n",
    "\n",
    "\n",
    "def cross_entropy(_input, target, size_average=True):\n",
    "    \"\"\" Cross entropy that accepts soft targets\n",
    "    Args:\n",
    "         pred: predictions for neural network\n",
    "         targets: targets, can be soft\n",
    "         size_average: if false, sum is returned instead of mean\n",
    "    Examples::\n",
    "        _input = torch.FloatTensor([[1.1, 2.8, 1.3], [1.1, 2.1, 4.8]])\n",
    "        _input = torch.autograd.Variable(out, requires_grad=True)\n",
    "        target = torch.FloatTensor([[0.05, 0.9, 0.05], [0.05, 0.05, 0.9]])\n",
    "        target = torch.autograd.Variable(y1)\n",
    "        loss = cross_entropy(_input, target)\n",
    "        loss.backward()\n",
    "    \"\"\"\n",
    "    logsoftmax = torch.nn.LogSoftmax(-1)\n",
    "    if size_average:\n",
    "        return torch.mean(torch.sum(-target * logsoftmax(_input), dim=1))\n",
    "    else:\n",
    "        return torch.sum(torch.sum(-target * logsoftmax(_input), dim=1))\n",
    "\n",
    "\n",
    "class TwoOutputsNN(torch.nn.Module):\n",
    "    def __init__(self, d, m, q):\n",
    "        super(TwoOutputsNN, self).__init__()\n",
    "        self._lin1 = torch.nn.Linear(d, m)\n",
    "        self._act1 = torch.nn.ReLU()\n",
    "        self._lin2 = torch.nn.Linear(m, m)\n",
    "        self._act2 = torch.nn.ReLU()\n",
    "        self._lin3 = torch.nn.Linear(m, q)\n",
    "        self._out_softmax = torch.nn.Softmax(-1)\n",
    "        \n",
    "        self._queue = [\n",
    "            self._lin1,\n",
    "            self._act1,\n",
    "            self._lin2,\n",
    "            self._act2,\n",
    "            self._lin3\n",
    "        ]\n",
    "        \n",
    "    def forward(self, x):\n",
    "        result = x\n",
    "        for layer in self._queue:\n",
    "            result = layer(result)\n",
    "        \n",
    "        out1 = self._out_softmax(result)\n",
    "        out2 = result\n",
    "        return out1, out2\n",
    "    \n",
    "def get_teacher(d, m, q):\n",
    "    model = TwoOutputsNN(d, m, q)\n",
    "    opt = torch.optim.RMSprop(model.parameters(), lr=1e-3)\n",
    "    \n",
    "    loss = cross_entropy\n",
    "    \n",
    "    return (model, opt, loss)\n",
    "\n",
    "\n",
    "def Dist(d, m, q, L):\n",
    "    def _hard_loss(_input, target, L):\n",
    "        return (1.-L)*cross_entropy(_input, target)\n",
    "\n",
    "    def _soft_loss(_input, target, L):\n",
    "        return L*cross_entropy(_input, target)\n",
    "\n",
    "    \n",
    "    model = torch.nn.Sequential()\n",
    "    model.add_module('d1', torch.nn.Linear(d, m))\n",
    "    model.add_module('a1', torch.nn.ReLU())\n",
    "    model.add_module('d2', torch.nn.Linear(m, m))\n",
    "    model.add_module('a2', torch.nn.ReLU())\n",
    "    model.add_module('d3', torch.nn.Linear(m, q))\n",
    "    model.add_module('a3', torch.nn.Softmax(-1))\n",
    "    \n",
    "    hard_loss = _hard_loss\n",
    "    soft_loss = _soft_loss\n",
    "\n",
    "    \n",
    "#     opt = torch.optim.RMSprop(model.parameters(), lr=1e-3)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    \n",
    "    \n",
    "    return (model, opt, hard_loss, soft_loss)\n",
    "\n",
    "\n",
    "def _get_iterator_wrapper(iterable, _tqdm=False, leave=False, desc=''):\n",
    "    if _tqdm:\n",
    "        return tqdm.tqdm(iterable, leave=leave, desc=desc)\n",
    "    else:\n",
    "        return iterable\n",
    "\n",
    "\n",
    "def main():\n",
    "    saved_filename = 'saved_objects_mnist_{}.pcl'.format(N)\n",
    "    from_save = torch.load(saved_filename)\n",
    "\n",
    "    batch_size = 50\n",
    "    m = 20\n",
    "    q = 10\n",
    "    d = from_save['transformed_data_train'][0][0].shape[0]\n",
    "\n",
    "    model_predictions = from_save['model_predictions']\n",
    "    logfile_name = 'torch_version_logs/mnist_{}/new_log_mnist_{}.txt'.format(N, num_try)\n",
    "    iofile = open(logfile_name, 'w')\n",
    "\n",
    "    for T in _get_iterator_wrapper([1,2,5,10,20,50], _tqdm=False, desc='T_loop'):\n",
    "        print('{}, {} started with T = {}'.format(N, num_try, T))\n",
    "\n",
    "        for L in _get_iterator_wrapper([0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0], _tqdm=False, leave=False, desc='L_loop'):                \n",
    "            labels_soften = F.softmax(model_predictions/T, -1)\n",
    "            student, student_opt, hard_loss, soft_loss = Dist(d, m, q, L)\n",
    "            student.double()\n",
    "            student.cuda()\n",
    "\n",
    "            _t = _get_iterator_wrapper(np.arange(n_epochs))\n",
    "            for epoch in _t:\n",
    "                cur_start = 0\n",
    "                loss_hist = []\n",
    "                for batch, label in _get_iterator_wrapper(from_save['transformed_dataloader_train']):\n",
    "            #         print(cur_start, batch_size, labels_soften[cur_start:cur_start+batch_size].shape)\n",
    "                    batch = batch.cuda()\n",
    "                    label = label.cuda()\n",
    "                    # Step 1. Remember that PyTorch accumulates gradients.\n",
    "                    # We need to clear them out before each instance\n",
    "                    student.zero_grad()\n",
    "\n",
    "                    predictions = student(batch)\n",
    "                    hard_loss = cross_entropy(predictions, label)\n",
    "                    soft_loss = cross_entropy(predictions, labels_soften[cur_start:cur_start+batch_size].cuda())\n",
    "\n",
    "                    total_loss = (1.-L)*hard_loss + L*soft_loss\n",
    "                    total_loss.backward()\n",
    "            #                 soft_loss.backward()\n",
    "    #                 loss_hist.append(np.mean(np.array(total_loss.detach())))\n",
    "\n",
    "\n",
    "                    student_opt.step()\n",
    "                    cur_start += batch_size\n",
    "\n",
    "    #             if epoch % 25 == 0:\n",
    "    #                 val_acc = get_predictions_on_dataset(student, from_save['transformed_data_val'], compute_accuracy=True)[-1]\n",
    "    #                 print(val_acc)\n",
    "    #             _t.set_postfix(val_acc=val_acc, mean_loss = np.mean(loss_hist[-50:]))\n",
    "\n",
    "            acc_student = get_predictions_on_dataset(student, from_save['transformed_data_test'], compute_accuracy=True)[-1]\n",
    "            iofile.write(str([N, T, L, acc_student])+'\\n')\n",
    "\n",
    "    iofile.close()\n",
    "\n",
    "    return 'Process with N = {}, num_try = {}, n_epochs = {} finished. Log file {}'.format(N, num_try, n_epochs, logfile_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'N' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-03a0aed881da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-e8ad04a13e51>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m     \u001b[0msaved_filename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'saved_objects_mnist_{}.pcl'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m     \u001b[0mfrom_save\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaved_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'N' is not defined"
     ]
    }
   ],
   "source": [
    "res = main()\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
